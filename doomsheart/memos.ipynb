{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read Image\n",
    "im = face_list[5]\n",
    "size = im.shape\n",
    "\n",
    "#2D image points. If you change the image, you need to change vector\n",
    "image_points = np.array([\n",
    "                            (359, 391),     # Nose tip\n",
    "                            (399, 561),     # Chin\n",
    "                            (337, 297),     # Left eye left corner\n",
    "                            (513, 301),     # Right eye right corne\n",
    "                            (345, 465),     # Left Mouth corner\n",
    "                            (453, 469)      # Right mouth corner\n",
    "                        ], dtype=\"double\")\n",
    " \n",
    "# 3D model points.\n",
    "model_points = np.array([\n",
    "                            (0.0, 0.0, 0.0),             # Nose tip\n",
    "                            (0.0, -330.0, -65.0),        # Chin\n",
    "                            (-225.0, 170.0, -135.0),     # Left eye left corner\n",
    "                            (225.0, 170.0, -135.0),      # Right eye right corne\n",
    "                            (-150.0, -150.0, -125.0),    # Left Mouth corner\n",
    "                            (150.0, -150.0, -125.0)      # Right mouth corner\n",
    "                         \n",
    "                        ])\n",
    "\n",
    "# Camera internals\n",
    " \n",
    "focal_length = size[1]\n",
    "center = (size[1]/2, size[0]/2)\n",
    "camera_matrix = np.array(\n",
    "                         [[focal_length, 0, center[0]],\n",
    "                         [0, focal_length, center[1]],\n",
    "                         [0, 0, 1]], dtype = \"double\"\n",
    "                         )\n",
    " \n",
    "print(\"Camera Matrix :\\n {0}\".format(camera_matrix))\n",
    " \n",
    "dist_coeffs = np.zeros((4,1)) # Assuming no lens distortion\n",
    "(success, rotation_vector, translation_vector) = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.CV_ITERATIVE)\n",
    " \n",
    "print (\"Rotation Vector:\\n {0}\".format(rotation_vector))\n",
    "print (\"Translation Vector:\\n {0}\".format(translation_vector))\n",
    "\n",
    "(nose_end_point2D, jacobian) = cv2.projectPoints(np.array([(0.0, 0.0, 1000.0)]), rotation_vector, translation_vector, camera_matrix, dist_coeffs)\n",
    "print(image_points)\n",
    "for p in image_points:\n",
    "    cv2.circle(im, (int(p[0]), int(p[1])), 30, (0,255,255), -1)\n",
    " \n",
    " \n",
    "p1 = ( int(image_points[0][0]), int(image_points[0][1]))\n",
    "p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))\n",
    " \n",
    "cv2.line(im, p1, p2, (255,0,0), 2)\n",
    " \n",
    "# Display image\n",
    "print(p1)\n",
    "print(p2)\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-p\", \"--shape-predictor\", required=True,\n",
    "\thelp=\"path to facial landmark predictor\")\n",
    "args = vars(ap.parse_args())\n",
    "# initialize dlib's face detector (HOG-based) and then create the\n",
    "# facial landmark predictor\n",
    "print(\"[INFO] loading facial landmark predictor...\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(args[\"shape_predictor\"])\n",
    " \n",
    "# initialize the video stream and sleep for a bit, allowing the\n",
    "# camera sensor to warm up\n",
    "print(\"[INFO] camera sensor warming up...\")\n",
    "vs = VideoStream(src=1).start()\n",
    "# vs = VideoStream(usePiCamera=True).start() # Raspberry Pi\n",
    "time.sleep(2.0)\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "\t# grab the frame from the threaded video stream, resize it to\n",
    "\t# have a maximum width of 400 pixels, and convert it to\n",
    "\t# grayscale\n",
    "\tframe = vs.read()\n",
    "\tframe = imutils.resize(frame, width=400)\n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "\t# detect faces in the grayscale frame\n",
    "\trects = detector(gray, 0)\n",
    " \n",
    "\t# check to see if a face was detected, and if so, draw the total\n",
    "\t# number of faces on the frame\n",
    "\tif len(rects) > 0:\n",
    "\t\ttext = \"{} face(s) found\".format(len(rects))\n",
    "\t\tcv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "\t\t\t0.5, (0, 0, 255), 2)\n",
    "        \t# loop over the face detections\n",
    "\tfor rect in rects:\n",
    "\t\t# compute the bounding box of the face and draw it on the\n",
    "\t\t# frame\n",
    "\t\t(bX, bY, bW, bH) = face_utils.rect_to_bb(rect)\n",
    "\t\tcv2.rectangle(frame, (bX, bY), (bX + bW, bY + bH),\n",
    "\t\t\t(0, 255, 0), 1)\n",
    " \n",
    "\t\t# determine the facial landmarks for the face region, then\n",
    "\t\t# convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "\t\t# array\n",
    "\t\tshape = predictor(gray, rect)\n",
    "\t\tshape = face_utils.shape_to_np(shape)\n",
    " \n",
    "\t\t# loop over the (x, y)-coordinates for the facial landmarks\n",
    "\t\t# and draw each of them\n",
    "\t\tfor (i, (x, y)) in enumerate(shape):\n",
    "\t\t\tcv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "\t\t\tcv2.putText(frame, str(i + 1), (x - 10, y - 10),\n",
    "\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 255), 1)\n",
    "            \t# show the frame\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "\t# if the `q` key was pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    " \n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation = cv.getRotationMatrix2D((40, 40), 5, 1) # 1은 확대/축소값입니다.\n",
    "result = cv.warpAffine(face_list[10], rotation, (face_list[10].shape[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compactness = []\n",
    "kvals = np.arange(1,10)\n",
    "for k in tqdm(kvals):\n",
    "    c, _, _ =  cv.kmeans(face_list, k, None, criteria, 10, flags)\n",
    "    compactness.append(c)\n",
    "plt.plot(kvals, compactness, 'o-', linewidth=4,\n",
    "        markersize=12)\n",
    "plt.xlabel('number of clusters')\n",
    "plt.ylabel('compactness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "compactness, clusters, centers = cv.kmeans(face_list, k, None, criteria, 10, flags)\n",
    "make_directory(SAVE_CROPPED_FACE_DIR + \"_cluster_\" + str(k))\n",
    "for i in range(k):\n",
    "    make_directory(SAVE_CROPPED_FACE_DIR + \"_cluster_\" + str(k) + '/' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f_n, c_k in tqdm(zip(face_files, clusters)):\n",
    "    print(f_n, c_k )\n",
    "    shutil.copy2(SAVE_CROPPED_FACE_DIR + '/' + f_n, SAVE_CROPPED_FACE_DIR + \"_cluster_\" + str(k) + '/' + str(c_k[0]) + '/' + f_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(face_list):\n",
    "     encodings = face_list\n",
    "\n",
    "     clt = DBSCAN(metric=\"euclidean\")\n",
    "     clt.fit(encodings)\n",
    "\n",
    "     label_ids = np.unique(clt.labels_)\n",
    "     num_unique_faces = len(np.where(label_ids > -1)[0])\n",
    "\n",
    "     for label_id in label_ids:\n",
    "         dir_name = \"drive/ID%d\" % label_id\n",
    "         os.mkdir(dir_name)\n",
    "\n",
    "         indexes = np.where(clt.labels_ == label_id)[0]\n",
    "\n",
    "         for i in  tqdm(indexes):\n",
    "             frame_id = face_list[i].frame_id\n",
    "             box = face_list[i].box\n",
    "             pathname = os.path.join(self.capture_dir,\n",
    "                                     self.capture_filename(frame_id))\n",
    "             image = cv2.imread(pathname)\n",
    "             face_image = self.getFaceImage(image, box)\n",
    "             filename = dir_name + \"-\" + self.capture_filename(frame_id)\n",
    "             pathname = os.path.join(dir_name, filename)\n",
    "             cv2.imwrite(pathname, face_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "clt = DBSCAN(metric=\"euclidean\")\n",
    "clt.fit(face_list)\n",
    "\n",
    "label_ids = np.unique(clt.labels_)\n",
    "num_unique_faces = len(np.where(label_ids > -1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "def rect_to_bb(rect):\n",
    "\t# take a bounding predicted by dlib and convert it\n",
    "\t# to the format (x, y, w, h) as we would normally do\n",
    "\t# with OpenCV\n",
    "\tx = rect.left()\n",
    "\ty = rect.top()\n",
    "\tw = rect.right() - x\n",
    "\th = rect.bottom() - y\n",
    " \n",
    "\t# return a tuple of (x, y, w, h)\n",
    "\treturn (x, y, w, h)\n",
    "\n",
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "\t# initialize the list of (x, y)-coordinates\n",
    "\tcoords = np.zeros((68, 2), dtype=dtype)\n",
    " \n",
    "\t# loop over the 68 facial landmarks and convert them\n",
    "\t# to a 2-tuple of (x, y)-coordinates\n",
    "\tfor i in range(0, 68):\n",
    "\t\tcoords[i] = (shape.part(i).x, shape.part(i).y)\n",
    " \n",
    "\t# return the list of (x, y)-coordinates\n",
    "\treturn coords\n",
    "\n",
    "\n",
    " \n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-p\", \"--shape-predictor\", required=True,\n",
    "\thelp=\"path to facial landmark predictor\")\n",
    "ap.add_argument(\"-i\", \"--image\", required=True,\n",
    "\thelp=\"path to input image\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(args[\"shape_predictor\"])\n",
    "\n",
    "# load the input image, resize it, and convert it to grayscale\n",
    "image = cv2.imread(args[\"image\"])\n",
    "image = imutils.resize(image, width=500)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "# detect faces in the grayscale image\n",
    "rects = detector(gray, 1)\n",
    "\n",
    "# loop over the face detections\n",
    "for (i, rect) in enumerate(rects):\n",
    "\t# determine the facial landmarks for the face region, then\n",
    "\t# convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "\t# array\n",
    "\tshape = predictor(gray, rect)\n",
    "\tshape = face_utils.shape_to_np(shape)\n",
    " \n",
    "\t# convert dlib's rectangle to a OpenCV-style bounding box\n",
    "\t# [i.e., (x, y, w, h)], then draw the face bounding box\n",
    "\t(x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "\tcv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    " \n",
    "\t# show the face number\n",
    "\tcv2.putText(image, \"Face #{}\".format(i + 1), (x - 10, y - 10),\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    " \n",
    "\t# loop over the (x, y)-coordinates for the facial landmarks\n",
    "\t# and draw them on the image\n",
    "\tfor (x, y) in shape:\n",
    "\t\tcv2.circle(image, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "# show the output image with the face detections + facial landmarks\n",
    "cv2.imshow(\"Output\", image)\n",
    "cv2.waitKey(0)\n",
    "# 하아앙앙ㅇㅇ아앙앙아앙앙앙앙아앙아아앙앙아ㅏ아아아아아아아아아아앙아아안앙아아아아아아아아아아아니안이ㅏㅇ니ㅏㅇ난안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import dlib\n",
    "import cv2\n",
    "import openface\n",
    "\n",
    "# https://github.com/scipy/scipy/issues/5995\n",
    "# install problem\n",
    "\n",
    "# You can download the required pre-trained face detection model here:\n",
    "# http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "predictor_model = RES_DIR + \"shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "# Take the image file name from the command line\n",
    "# file_name = sys.argv[1]\n",
    "\n",
    "# Create a HOG face detector using the built-in dlib class\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "face_pose_predictor = dlib.shape_predictor(predictor_model)\n",
    "face_aligner = openface.AlignDlib(predictor_model)\n",
    "\n",
    "# Take the image file name from the command line\n",
    "file_name = RES_DIR + 'redvelvet.jpg'\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(file_name)\n",
    "\n",
    "# Run the HOG face detector on the image data\n",
    "detected_faces = face_detector(image, 1)\n",
    "\n",
    "print(\"Found {} faces in the image file {}\".format(len(detected_faces), file_name))\n",
    "\n",
    "# Loop through each face we found in the image\n",
    "for i, face_rect in enumerate(detected_faces):\n",
    "\n",
    "\t# Detected faces are returned as an object with the coordinates \n",
    "\t# of the top, left, right and bottom edges\n",
    "\tprint(\"- Face #{} found at Left: {} Top: {} Right: {} Bottom: {}\".format(i, face_rect.left(), face_rect.top(), face_rect.right(), face_rect.bottom()))\n",
    "\n",
    "\t# Get the the face's pose\n",
    "\tpose_landmarks = face_pose_predictor(image, face_rect)\n",
    "\n",
    "\t# Use openface to calculate and perform the face alignment\n",
    "\talignedFace = face_aligner.align(534, image, face_rect, landmarkIndices=openface.AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "\n",
    "\t# Save the aligned image to a file\n",
    "plt.imshow(\"aligned_face_{}.jpg\".format(i), alignedFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install dlib\n",
    "!pip install opencv-python\n",
    "!pip install opencv-contrib-python\n",
    "!pip install --upgrade imutils\n",
    "!pip install cmake\n",
    "!pip install dlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import shutil\n",
    "\n",
    "RES_DIR = 'drive/My Drive/FaceClusterer/res/'\n",
    "CASCADE_DIR = RES_DIR + 'lbpcascade_frontalface_improved.xml'\n",
    "VIDEO_DIR = RES_DIR + 'vid/'\n",
    "RESULT_DIR = 'drive/My Drive/FaceClusterer/result/'\n",
    "PROGRAM_START_TIME = '20180827'\n",
    "VIDEO_FILE_NAME = 'PowerUp.mp4'\n",
    "SAVE_CROPPED_FACE_DIR = RESULT_DIR + 'Cropped_imgs/' + PROGRAM_START_TIME + '-' + VIDEO_FILE_NAME.split('.')[0]\n",
    "# PROGRAM_START_TIME = datetime.datetime.now().strftime('%Y%m%d_%H-%M-%S')\n",
    "# PROGRAM_START_TIME = datetime.datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "flags = cv.KMEANS_RANDOM_CENTERS\n",
    "\n",
    "def make_directory(_directory_name):\n",
    "    if not os.path.exists(_directory_name):\n",
    "        os.makedirs(_directory_name)\n",
    "        \n",
    "def get_area_of_frame_face_recognition(img, face_cascade):\n",
    "    grayed_img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    # def detectMultiScale(self, image, scaleFactor=None, minNeighbors=None, flags=None, minSize=None, maxSize=None)\n",
    "    face_area = face_cascade.detectMultiScale(image=grayed_img,scaleFactor=1.3,minNeighbors=5)\n",
    "    return face_area\n",
    "  \n",
    "def get_frame(video_capture, frame):\n",
    "    video_capture.set(cv.CAP_PROP_POS_FRAMES, frame)\n",
    "    ret, img = video_capture.read()\n",
    "    return img, frame, ret\n",
    "\n",
    "def save_cropped_img(img, faces, frame):\n",
    "    i = 0\n",
    "    if len(faces) != 0:\n",
    "        for (x, y, w, h) in faces:\n",
    "            iter = 0.2         \n",
    "            while True:\n",
    "                try:\n",
    "                    face_img = img[int(y - h * iter): int(y + (1 + iter) * h),\n",
    "                                   int(x - w * iter): int(x + (1 + iter) * w)]\n",
    "                    if int(y - h * iter) < 0 or int(x - w * iter) < 0:\n",
    "                        raise ValueError('range is false')\n",
    "                    break\n",
    "                except:\n",
    "                    iter = iter * 0.8\n",
    "                  #             cv.imwrite(SAVE_CROPPED_FACE_DIR + \"/\" + str(frame) + \"_\" + str(i) + \".jpg\", img[y:y + h, x: x + w])\n",
    "            try:\n",
    "                cv.imwrite(SAVE_CROPPED_FACE_DIR + \"/\" + str(frame) + \"_\" + str(i) + \".jpg\", cv.resize(face_img, (96, 96), interpolation=cv.INTER_AREA))\n",
    "            except:\n",
    "                print(int(y - h * iter))\n",
    "                print(int(x - w * iter))\n",
    "            i += 1\n",
    "\n",
    "# this function show image and quit when press q or end\n",
    "def show_img(img, faces):\n",
    "    if len(faces) != 0:\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 1)\n",
    "    cv.imshow('hello', img)\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "      \n",
    "def ORB():\n",
    "    img = cv.imread(SAVE_CROPPED_FACE_DIR + '/130_0.jpg')\n",
    "    imgray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img2 = None\n",
    "    orb = cv.ORB_create()\n",
    "    kp, des = orb.detectAndCompute(img, None)\n",
    "#     img2 = cv.drawKeypoints(img, kp, img2, (0, 0, 255), flags=0)\n",
    "#     plt.imshow(img2)\n",
    "    return kp, des \n",
    "\n",
    "def HarrisCorner():\n",
    "    img = cv.imread(SAVE_CROPPED_FACE_DIR + '/130_0.jpg')\n",
    "    img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    corners = cv.cornerHarris(img_gray, 2, 3, 0.04)\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     plt.imshow(corners, cmap='gray')\n",
    "    return coners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_CROPPED_FACE_DIR = RESULT_DIR + 'Cropped_imgs/' + PROGRAM_START_TIME + '-' + VIDEO_FILE_NAME.split('.')[0]\n",
    "make_directory(SAVE_CROPPED_FACE_DIR)\n",
    "\n",
    "VIDEO_FILE_NAME = 'PowerUp.mp4'\n",
    "m_video_capture = cv.VideoCapture(VIDEO_DIR + VIDEO_FILE_NAME)\n",
    "\n",
    "CASCADE_DIR = RES_DIR + 'lbpcascade_frontalface_improved.xml'\n",
    "m_face_cascade = cv.CascadeClassifier(CASCADE_DIR)\n",
    "\n",
    "TOTAL_FRAME = m_video_capture.get(cv.CAP_PROP_FRAME_COUNT)\n",
    "FRAME_STEP = 10\n",
    "LOAD_FAIL = []\n",
    "if len(os.listdir(SAVE_CROPPED_FACE_DIR)) == 0:\n",
    "    for frame in tqdm(range(0, int(TOTAL_FRAME), FRAME_STEP)):\n",
    "        img, frame, ret = get_frame(video_capture=m_video_capture, frame=frame)\n",
    "        if not ret:\n",
    "            LOAD_FAIL.append(frame)\n",
    "            continue\n",
    "        faces_area = get_area_of_frame_face_recognition(img=img, face_cascade=m_face_cascade)\n",
    "        save_cropped_img(img, faces_area, frame)\n",
    "    if len(LOAD_FAIL) > 0:\n",
    "        print(\"Fail to load %d of frame(s)\" % len(LOAD_FAIL))\n",
    "        print(LOAD_FAIL)\n",
    "else:    \n",
    "    print(\"Directory is not empty\")\n",
    "    print(\"Already extracted in %s\" % PROGRAM_START_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "import dlib\n",
    "# from skimage import io\n",
    "import cv2\n",
    "# You can download the required pre-trained face detection model here:\n",
    "# http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "predictor_model = RES_DIR + \"shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "# Take the image file name from the command line\n",
    "file_name = RES_DIR + 'img/redvelvet.jpg'\n",
    "\n",
    "# Create a HOG face detector using the built-in dlib class\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "face_pose_predictor = dlib.shape_predictor(predictor_model)\n",
    "\n",
    "win = dlib.image_window()\n",
    "\n",
    "# Take the image file name from the command line\n",
    "# file_name = sys.argv[1]\n",
    "\n",
    "# Load the image\n",
    "image = cv.cvtColor(cv.imread(file_name), cv.COLOR_BGR2RGB)\n",
    "\n",
    "# Run the HOG face detector on the image data\n",
    "detected_faces = face_detector(image, 1)\n",
    "\n",
    "print(\"Found {} faces in the image file {}\".format(len(detected_faces), file_name))\n",
    "\n",
    "# Show the desktop window with the image\n",
    "win.set_image(image)\n",
    "\n",
    "# Loop through each face we found in the image\n",
    "for i, face_rect in enumerate(detected_faces):\n",
    "\n",
    "    # Detected faces are returned as an object with the coordinates \n",
    "    # of the top, left, right and bottom edges\n",
    "    print(\"- Face #{} found at Left: {} Top: {} Right: {} Bottom: {}\".format(i, face_rect.left(), face_rect.top(), face_rect.right(), face_rect.bottom()))\n",
    "\n",
    "    # Draw a box around each face we found\n",
    "    win.add_overlay(face_rect)\n",
    "\n",
    "    # Get the the face's pose\n",
    "    pose_landmarks = face_pose_predictor(image, face_rect)\n",
    "\n",
    "    # Draw the face landmarks on the screen.\n",
    "    win.add_overlay(pose_landmarks)  \n",
    "    # \tprint(pose_landmarks)\n",
    "dlib.hit_enter_to_continue()\n",
    "\n",
    "\n",
    "face_file_path = file_name\n",
    "predictor_path = predictor_model\n",
    "\n",
    "# Load all the models we need: a detector to find the faces, a shape predictor\n",
    "# to find face landmarks so we can precisely localize the face\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "sp = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# Load the image using Dlib\n",
    "img = dlib.load_rgb_image(face_file_path)\n",
    "\n",
    "# Ask the detector to find the bounding boxes of each face. The 1 in the\n",
    "# second argument indicates that we should upsample the image 1 time. This\n",
    "# will make everything bigger and allow us to detect more faces.\n",
    "dets = detector(img, 1)\n",
    "\n",
    "num_faces = len(dets)\n",
    "if num_faces == 0:\n",
    "    print(\"Sorry, there were no faces found in '{}'\".format(face_file_path))\n",
    "    exit()\n",
    "\n",
    "# Find the 5 face landmarks we need to do the alignment.\n",
    "faces = dlib.full_object_detections()\n",
    "for detection in dets:\n",
    "    faces.append(sp(img, detection))\n",
    "\n",
    "window = dlib.image_window()\n",
    "\n",
    "# Get the aligned face images\n",
    "# Optionally: \n",
    "# images = dlib.get_face_chips(img, faces, size=160, padding=0.25)\n",
    "images = dlib.get_face_chips(img, faces, size=320)\n",
    "for image in images:\n",
    "    window.set_image(image)\n",
    "    dlib.hit_enter_to_continue()\n",
    "\n",
    "# It is also possible to get a single chip\n",
    "im_list = []\n",
    "for face in faces:\n",
    "    im_list.append(dlib.get_face_chip(img, face))\n",
    "print(im_list)\n",
    "\n",
    "window.set_image(image)\n",
    "dlib.hit_enter_to_continue()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
